{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import gensim\n",
    "import numpy as np\n",
    "import transformers\n",
    "from typing import Callable\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"Embeddings/GoogleNews-vectors-negative300.bin.gz\"     # Word2Vec\n",
    "embedding = gensim.models.KeyedVectors.load_word2vec_format(model, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Divyajeet Singh\\AppData\\Local\\Temp\\ipykernel_4932\\4275181089.py:5: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(\"Embeddings/glove.42B.300d/glove.42B.300d.txt\", target)\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "target = get_tmpfile(\"temp.txt\")\n",
    "glove2word2vec(\"Embeddings/glove.42B.300d/glove.42B.300d.txt\", target)\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(target)\n",
    "\n",
    "model.save_word2vec_format(\"Embeddings/glove.42B.300d.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"Embeddings/glove.42B.300d.bin.gz\"                       # GloVe\n",
    "embedding = gensim.models.KeyedVectors.load_word2vec_format(model, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"Embeddings/cc.en.300.bin.gz\"                          # FastText\n",
    "embedding = gensim.models.fasttext.load_facebook_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset: int, embeddings: str):\n",
    "    \"\"\"\n",
    "    Loads the given dataset and returns the train, test, and validation\n",
    "    sets with the corresponding labels.\n",
    "    :params:\n",
    "        - dataset: The dataset to load. Must be 1 or 2.\n",
    "        - embeddings: The word-embeddings to use. Must be \"Word2Vec\", \"GloVe\", or \"FastText\" (case-insensitive).\n",
    "    \"\"\"\n",
    "\n",
    "    embeddings_err = \"Invalid embeddings. Must be 'Word2Vec', 'GloVe', or 'FastText'.\"\n",
    "    dataset_err = \"Invalid dataset number. Must be 1 or 2.\"\n",
    "    embeddings = embeddings.casefold()\n",
    "\n",
    "    assert dataset in [1, 2], dataset_err\n",
    "    assert embeddings in [\"word2vec\", \"glove\", \"fasttext\"], embeddings_err\n",
    "\n",
    "    path = rf\"Assignment-2/Datasets/processed/dataset_{dataset}\"\n",
    "    train_path, test_path, val_path = [os.path.join(path, f\"{x}.json\") for x in [\"train\", \"test\", \"val\"]]\n",
    "\n",
    "    with open(train_path) as train, open(test_path) as test, open(val_path) as val:\n",
    "        train_data = json.load(train)\n",
    "        test_data = json.load(test)\n",
    "        val_data = json.load(val)\n",
    "    print(\"HERE-1\")\n",
    "\n",
    "    if embeddings == \"word2vec\":\n",
    "        model = \"Assignment-2/Embeddings/GoogleNews-vectors-negative300.bin.gz\"\n",
    "        embedding = gensim.models.KeyedVectors.load_word2vec_format(model, binary=True)\n",
    "    elif embeddings == \"glove\":\n",
    "        model = \"Assignment-2/Embeddings/glove.42B.300d.bin.gz\"\n",
    "        embedding = gensim.models.KeyedVectors.load_word2vec_format(model, binary=True)\n",
    "    else:\n",
    "        model = \"Assignment-2/Embeddings/cc.en.300.bin.gz\"\n",
    "        embedding = gensim.models.fasttext.load_facebook_model(model)\n",
    "    print(\"HERE-2\")\n",
    "\n",
    "    LABELS = set()\n",
    "    for key, data in train_data.items():\n",
    "        LABELS.update(data[\"labels\"])\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(sorted(LABELS))\n",
    "    print(\"HERE-3\")\n",
    "\n",
    "    TRAIN_DATA, TRAIN_LABELS = [], []\n",
    "    for _, data in train_data.items():\n",
    "        TRAIN_DATA.append(data[\"text\"].split())\n",
    "        TRAIN_LABELS.append(data[\"labels\"])\n",
    "    print(\"HERE-4\")\n",
    "\n",
    "    TEST_DATA, TEST_LABELS = [], []\n",
    "    for _, data in test_data.items():\n",
    "        TEST_DATA.append(data[\"text\"].split())\n",
    "        TEST_LABELS.append(data[\"labels\"])\n",
    "    print(\"HERE-5\")\n",
    "\n",
    "    VAL_DATA, VAL_LABELS = [], []\n",
    "    for _, data in val_data.items():\n",
    "        VAL_DATA.append(data[\"text\"].split())\n",
    "        VAL_LABELS.append(data[\"labels\"])\n",
    "    print(\"HERE-6\")\n",
    "\n",
    "    # train_set = CustomDataset(TRAIN_DATA, TRAIN_LABELS, encoder, vectorizor)\n",
    "    # test_set = CustomDataset(TEST_DATA, TEST_LABELS, encoder, vectorizor)\n",
    "    # val_set = CustomDataset(VAL_DATA, VAL_LABELS, encoder, vectorizor)\n",
    "\n",
    "    # return train_set, test_set, val_set\n",
    "\n",
    "    return (TRAIN_DATA, TRAIN_LABELS), (TEST_DATA, TEST_LABELS), (VAL_DATA, VAL_LABELS), encoder, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE-1\n",
      "HERE-2\n",
      "HERE-3\n",
      "HERE-4\n",
      "HERE-5\n",
      "HERE-6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "((train_data, train_labels),\n",
    " (test_data, test_labels),\n",
    " (val_data, val_labels),\n",
    " encoder, embedding, vectorizor) = load_dataset(1, \"WORD2VEC\")\n",
    "\n",
    "try:\n",
    "    embedding_matrix = torch.FloatTensor(embedding.vectors)\n",
    "    vectorizor = embedding.get_vector\n",
    "except AttributeError:\n",
    "    embedding_matrix = torch.FloatTensor(embedding.wv.vectors)\n",
    "    vectorizor = lambda x: embedding.wv[x]\n",
    "\n",
    "embedding_matrix = torch.cat([embedding_matrix, torch.zeros(1, embedding_matrix.shape[1])])\n",
    "embedding.key_to_index[\"<UNK>\"] = len(embedding.key_to_index)\n",
    "embedding.index_to_key.append(\"<UNK>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 300])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDING_LAYER = torch.nn.Embedding.from_pretrained(embedding_matrix)\n",
    "word_embedding = EMBEDING_LAYER(torch.tensor([embedding.key_to_index.get(word) for word in \"the quick brown fox\".split()]))\n",
    "word_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'was', 'also', 'asked', 'whether', 'Agya', '<span', 'class=\"hidden_text\"', 'id=\"span_5\">', 'CRA', 'No.326-DB', 'of', '1998', '6</span>', 'Kaur,', 'mother-in-law', 'of', 'the', 'deceased', 'lived', 'separately', 'from', 'Tarlochan', 'Singh.']\n",
      "['5.2', 'CW3', 'Mr', 'Vijay', 'Mishra', ',', 'Deputy', 'Manager,', 'HDFC', 'Bank,', 'Noida,', 'UP', 'has', 'deposed', 'that', 'complainant', 'had', 'a', 'current', 'account', 'with', 'HDFC', 'Bank', 'in', 'the', 'year', '2004\\xad2005.']\n",
      "['You', 'are', 'hereby', 'asked', 'not', 'to', 'carry', 'out', 'any', 'construction', 'work', 'of', 'the', 'said', 'building', 'hereafter', 'since', 'the', 'agreement', 'has', 'been', 'terminated\".']\n",
      "['After', 'all', 'the', 'steps', 'at', 'the', 'stage', 'of', 'investigation', 'has', 'to', 'be', 'reported', 'before', 'the', 'Court', 'and', 'the', 'order', 'passed', 'thereon', 'is', 'obviously', 'judicial', 'order', 'and', 'this', 'takes', 'clear', 'note', 'of', 'the', 'agony', 'of', 'the', 'learned', 'Counsels.']\n",
      "['PW--2', 'Chandregowda', 'is', 'the', 'younger', 'brother', 'of', 'bi-hglllltleceased.']\n"
     ]
    }
   ],
   "source": [
    "subset = train_data[:5]\n",
    "for sentence in subset:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 [57, 10, 53, 438, 369, 817011, 3000000, 3000000, 3000000, 30909, 3000000, 3000000, 3000000, 3000000, 3000000, 3000000, 3000000, 11, 9895, 1958, 8019, 17, 1219623, 3000000]\n",
      "27 [3000000, 863288, 602, 30124, 33742, 3000000, 3562, 3000000, 38263, 3000000, 3000000, 13860, 24, 20696, 3, 21322, 35, 3000000, 403, 1201, 8, 38263, 669, 1, 11, 36, 3000000]\n",
      "22 [228, 19, 29224, 438, 13, 3000000, 1635, 49, 101, 984, 141, 3000000, 11, 9, 473, 65530, 140, 11, 729, 24, 42, 3000000]\n",
      "37 [361, 52, 11, 1830, 12, 11, 943, 3000000, 915, 24, 3000000, 16, 343, 99, 11, 1557, 3000000, 11, 555, 1126, 68081, 4, 2633, 5481, 555, 3000000, 28, 920, 645, 1734, 3000000, 11, 20599, 3000000, 11, 1638, 3000000]\n",
      "8 [3000000, 2165064, 4, 11, 2520, 1722, 3000000, 3000000]\n"
     ]
    }
   ],
   "source": [
    "subset = [[embedding.key_to_index.get(word, len(embedding.key_to_index)-1) for word in sentence] for sentence in subset]\n",
    "for sentence in subset:\n",
    "    print(len(sentence), sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 [57, 10, 53, 438, 369, 817011, 3000000, 3000000, 3000000, 30909, 3000000, 3000000, 3000000, 3000000, 3000000, 3000000, 3000000, 11, 9895, 1958, 8019, 17, 1219623, 3000000, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "37 [3000000, 863288, 602, 30124, 33742, 3000000, 3562, 3000000, 38263, 3000000, 3000000, 13860, 24, 20696, 3, 21322, 35, 3000000, 403, 1201, 8, 38263, 669, 1, 11, 36, 3000000, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "37 [228, 19, 29224, 438, 13, 3000000, 1635, 49, 101, 984, 141, 3000000, 11, 9, 473, 65530, 140, 11, 729, 24, 42, 3000000, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "37 [361, 52, 11, 1830, 12, 11, 943, 3000000, 915, 24, 3000000, 16, 343, 99, 11, 1557, 3000000, 11, 555, 1126, 68081, 4, 2633, 5481, 555, 3000000, 28, 920, 645, 1734, 3000000, 11, 20599, 3000000, 11, 1638, 3000000]\n",
      "37 [3000000, 2165064, 4, 11, 2520, 1722, 3000000, 3000000, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "padded = torch.nn.utils.rnn.pad_sequence([torch.tensor(sentence) for sentence in subset], batch_first=True)\n",
    "for row in padded:\n",
    "    x = (list(i.item() for i in row))\n",
    "    print(len(x), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 37, 300])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDING_LAYER(padded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for dataset_1 and dataset_2.\n",
    "    :attrs:\n",
    "        - data: The data of the dataset.\n",
    "        - targets: The targets of the dataset.\n",
    "        - encoder: The (already fitted) label-encoder for the targets.\n",
    "    \"\"\"\n",
    "\n",
    "    data: list[list[str]]\n",
    "    targets: list[list[str]]\n",
    "    encoder: LabelEncoder\n",
    "    embedding: \"Word-Embedding\"\n",
    "\n",
    "    def __init__(self, data, targets, encoder, embedding):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.encoder = encoder\n",
    "        self.embedding = embedding\n",
    "        self._convert_to_index()\n",
    "        self._pad()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the length of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns the data and target at the given index.\n",
    "        Converts each sentence into a tensor of word indices using\n",
    "        the chosen word-embeddings.\n",
    "        \"\"\"\n",
    "        return self.data[index], self.targets[index]\n",
    "\n",
    "    def _convert_to_index(self) -> None:\n",
    "        \"\"\"\n",
    "        Converts all sentences to their corresponding word-indices.\n",
    "        \"\"\"\n",
    "        unk = len(self.embedding.key_to_index)-1\n",
    "        self.data = [\n",
    "            [self.embedding.key_to_index.get(word, unk) for word in sentence]\n",
    "            for sentence in self.data\n",
    "        ]\n",
    "\n",
    "    def _pad(self) -> None:\n",
    "        \"\"\"\n",
    "        Pads all sentences to the maximum length.\n",
    "        \"\"\"\n",
    "        self.targets = [torch.LongTensor(self.encoder.transform(label)) for label in self.targets]\n",
    "        max_len = max(len(sentence) for sentence in self.data)\n",
    "        self.data = torch.nn.utils.rnn.pad_sequence([torch.tensor(sentence) for sentence in self.data], batch_first=True)\n",
    "        self.targets = torch.nn.utils.rnn.pad_sequence(self.targets, batch_first=True, padding_value=26)\n",
    "\n",
    "\n",
    "train_set = CustomDataset(train_data, train_labels, encoder, embedding)\n",
    "test_set = CustomDataset(test_data, test_labels, encoder, embedding)\n",
    "val_set = CustomDataset(val_data, val_labels, encoder, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([     57,      10,      53,     438,     369,  817011, 3000000, 3000000,\n",
       "         3000000,   30909, 3000000, 3000000, 3000000, 3000000, 3000000, 3000000,\n",
       "         3000000,      11,    9895,    1958,    8019,      17, 1219623, 3000000,\n",
       "               0,       0,       0,       0,       0,       0,       0,       0,\n",
       "               0,       0,       0,       0,       0,       0,       0,       0,\n",
       "               0,       0,       0,       0,       0,       0,       0,       0,\n",
       "               0,       0,       0,       0,       0,       0,       0,       0,\n",
       "               0,       0,       0,       0,       0,       0,       0,       0,\n",
       "               0,       0,       0,       0,       0,       0]),\n",
       " tensor([26, 26, 26, 26, 26,  6, 26, 26, 26, 26, 26, 26, 26, 26,  6, 26, 26, 26,\n",
       "         26, 26, 26, 26,  6, 19, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
       "         26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
       "         26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26]))"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collate_fn(batch):\n",
    "#     # Sort batch based on the length of sequences\n",
    "#     batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "#     # Extract sentences and labels from the batch\n",
    "#     sentences, labels = zip(*batch)\n",
    "#     # Pad sequences to the length of the longest sequence in the batch\n",
    "#     padded_sentences = torch.nn.utils.rnn.pad_sequence(sentences, batch_first=True, padding_value=0)\n",
    "#     # Create tensor for labels\n",
    "#     label_pad = encoder.transform([\"O\"])[0]\n",
    "#     padded_labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=label_pad)\n",
    "#     # Create mask tensor to mask out padded values\n",
    "#     mask = (padded_sentences != 0).float()\n",
    "#     return padded_sentences, padded_labels, mask\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 70]) torch.Size([32, 70])\n"
     ]
    }
   ],
   "source": [
    "for data, labels in train_loader:\n",
    "    print(data.shape, labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.47957302534960183\n",
      "Epoch 2 Loss: 0.39453179427827023\n",
      "Epoch 3 Loss: 0.3878739983199602\n",
      "Epoch 4 Loss: 0.36176555191140725\n",
      "Epoch 5 Loss: 0.3094607300136194\n",
      "Epoch 6 Loss: 0.26045605089085033\n",
      "Epoch 7 Loss: 0.22858167781060434\n",
      "Epoch 8 Loss: 0.206563933080886\n",
      "Epoch 9 Loss: 0.20605460183316493\n",
      "Epoch 10 Loss: 0.20290843658594496\n"
     ]
    }
   ],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    # input_size: batch x seq_len x input_size == 32 x seq_len x 300\n",
    "    # hidden_size: hidden_size hyperparam\n",
    "    # output_size: num_classes = 27\n",
    "    # num_layers: num_layers hyperparam\n",
    "\n",
    "    # ADD embedding layer and time distributed layer\n",
    "    #Embedding layer,LSTM layer,Time stribution layer\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, embedding_matrix):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(embedding_matrix)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        hidden = torch.zeros(self.num_layers, x.shape[0], self.hidden_size)\n",
    "        output, _ = self.rnn(x, hidden)\n",
    "        output = self.fc(output)\n",
    "        # return output                  # for crossentropy\n",
    "        return self.softmax(output)      # for NLLLoss\n",
    "\n",
    "\n",
    "model = RNN(300, 128, 2, 27, embedding_matrix)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for data, labels in train_loader:\n",
    "        output = model(data)\n",
    "        loss = criterion(output.permute(0, 2, 1), labels)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    total_loss /= len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} Loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 of 30\n",
      "tensor(2311.6667) tensor(148.7037) tensor(148.7037) tensor(63820.9258)\n",
      "Accuracy: 0.9955\n",
      "Precision: 0.3731\n",
      "Recall: 0.2833\n",
      "F1-Score: 0.3015\n"
     ]
    }
   ],
   "source": [
    "# Calculate true positives and negatives, and false positives and negatives\n",
    "\n",
    "model.eval()\n",
    "confusion_matrix = torch.zeros(27, 27)\n",
    "\n",
    "loader = test_loader\n",
    "with torch.no_grad():\n",
    "    for p, (data, labels) in enumerate(loader):\n",
    "        output = model(data)\n",
    "        pred = torch.argmax(output, dim=2)\n",
    "        for i in range(labels.shape[0]):\n",
    "            for j in range(labels.shape[1]):\n",
    "                confusion_matrix[labels[i][j], pred[i][j]] += 1\n",
    "        print(p, \"of\", len(loader), end=\"\\r\")\n",
    "\n",
    "print()\n",
    "# Calculate accuracy, precision, recall, and F1-score\n",
    "\n",
    "TP = torch.diag(confusion_matrix)\n",
    "FP = confusion_matrix.sum(dim=0) - TP\n",
    "FN = confusion_matrix.sum(dim=1) - TP\n",
    "TN = confusion_matrix.sum() - (TP + FP + FN)\n",
    "\n",
    "accuracy = (TP + TN) / (TP + FP + FN + TN)\n",
    "precision = TP / (TP + FP + 1e-6)\n",
    "recall = TP / (TP + FN + 1e-6)\n",
    "f1 = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "\n",
    "print(TP.mean(), FP.mean(), FN.mean(), TN.mean())\n",
    "\n",
    "print(f\"Accuracy: {accuracy.mean().item():.4f}\")\n",
    "print(f\"Precision: {precision.mean().item():.4f}\")\n",
    "print(f\"Recall: {recall.mean().item():.4f}\")\n",
    "print(f\"F1-Score: {f1.mean().item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
