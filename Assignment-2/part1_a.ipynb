{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##see later\n",
    "import re\n",
    "\n",
    "def add_spaces_around_punctuation(text):\n",
    "    # Define a regular expression pattern to match punctuation marks without surrounding spaces\n",
    "    pattern = r'(?<=[^\\s\\d])(?=[.,;:?!])|(?<=[.,;:?!])(?=[^\\s])'\n",
    "\n",
    "    # Add spaces around punctuation marks\n",
    "    modified_text = re.sub(pattern, ' ', text)\n",
    "\n",
    "    return modified_text\n",
    "\n",
    "def add_spaces_around_parentheses(text):\n",
    "    # Define a regular expression pattern to match parentheses without surrounding spaces\n",
    "    pattern = r'(?<=\\w)(\\()|(\\))(?=\\w)'\n",
    "\n",
    "    # Add spaces around parentheses\n",
    "    modified_text = re.sub(pattern, ' \\\\1\\\\2 ', text)\n",
    "\n",
    "    return modified_text\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    \"Kaur,\",\n",
    "    \"28.07.1988\",\n",
    "    \"(pf chang)\"\n",
    "]\n",
    "\n",
    "# Apply both transformations to each sentence\n",
    "modified_sentences = [add_spaces_around_punctuation(add_spaces_around_parentheses(sentence)) for sentence in sentences]\n",
    "\n",
    "# Print modified sentences\n",
    "for original, modified in zip(sentences, modified_sentences):\n",
    "    print(f\"Original: {original}\")\n",
    "    print(f\"Modified: {modified}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"NER/NER_TRAIN_JUDGEMENT.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open(\"NER/NER_TEST_JUDGEMENT.json\",\"r\") as f:\n",
    "    data_test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#SPlitting\n",
    "from skmultilearn.model_selection import iterative_train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "for i in range(len(data)):\n",
    "    for j in range(len(data[i][\"annotations\"][0][\"result\"])):\n",
    "        label_text = data[i][\"annotations\"][0][\"result\"][j][\"value\"][\"text\"]\n",
    "        if(label_text in labels):\n",
    "            continue\n",
    "        else:\n",
    "            labels[label_text] = data[i][\"annotations\"][0][\"result\"][j][\"value\"][\"labels\"][0]\n",
    "\n",
    "unique_labels = set()\n",
    "for text,keys in labels.items():\n",
    "    unique_labels.add(keys)\n",
    "\n",
    "indices = {key:value for key,value in zip(unique_labels,range(13))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(len(data))\n",
    "X = X.reshape(-1, 1)\n",
    "Y = np.zeros((len(data),13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    for j in range(len(data[i][\"annotations\"][0][\"result\"])):\n",
    "            Y[i][indices[data[i][\"annotations\"][0][\"result\"][j][\"value\"][\"labels\"][0]]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val = iterative_train_test_split(X, Y, test_size = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = [data[i[0]] for i in X_train]\n",
    "data_val = [data[i[0]] for i in X_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = []\n",
    "case_ids_train = []\n",
    "text_val = []\n",
    "case_ids_val = []\n",
    "text_test = []\n",
    "case_ids_test = []\n",
    "for i in range(len(data_train)):\n",
    "    text_train.append(data_train[i][\"data\"][\"text\"])\n",
    "    case_ids_train.append(data_train[i][\"id\"])\n",
    "\n",
    "for i in range(len(data_val)):\n",
    "    text_val.append(data_val[i][\"data\"][\"text\"])\n",
    "    case_ids_val.append(data_val[i][\"id\"])\n",
    "\n",
    "for i in range(len(data_test)):\n",
    "    text_test.append(data_test[i][\"data\"][\"text\"])\n",
    "    case_ids_test.append(data_test[i][\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def bio_label_partI(text,annotations):\n",
    "    tokens = text.split()\n",
    "    # print(tokens)\n",
    "    bio_temp = [\"O\" for _ in range(len(tokens))]\n",
    "    for annotation in annotations:\n",
    "        for result in annotation['result']:\n",
    "            start = result['value']['start']\n",
    "            end = result['value']['end']\n",
    "            entity_text = result['value']['text']\n",
    "            labels = result['value']['labels'][0]\n",
    "            \n",
    "            entities = entity_text.split()            \n",
    "\n",
    "            start_index = 0\n",
    "            curr_pos = 0\n",
    "            for i, token in enumerate(tokens):\n",
    "                curr_pos = text.find(token, curr_pos)\n",
    "                if curr_pos >= start:\n",
    "                    start_index = i\n",
    "                    break\n",
    "                curr_pos += len(token)\n",
    "\n",
    "            end_index = start_index+len(entities)-1           \n",
    "            try:\n",
    "                bio_temp[start_index] = \"B_\"+labels\n",
    "                for i in range(start_index+1,end_index+1):\n",
    "                    bio_temp[i] = \"I_\" + labels\n",
    "            except IndexError:\n",
    "                continue\n",
    "    return bio_temp    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIO_labels_train = []\n",
    "BIO_labels_val = []\n",
    "BIO_labels_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,line in enumerate(text_train):\n",
    "    BIO_labels_train.append(bio_label_partI(line,data_train[i][\"annotations\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,line in enumerate(text_test):\n",
    "    BIO_labels_test.append(bio_label_partI(line,data_test[i][\"annotations\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,line in enumerate(text_val):\n",
    "    BIO_labels_val.append(bio_label_partI(line,data_val[i][\"annotations\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "# Iterate over IDs, texts, and labels simultaneously\n",
    "for id_, text, label in zip(case_ids_test, text_test, BIO_labels_test):\n",
    "    data[id_] = {\n",
    "        \"text\": text,\n",
    "        \"labels\": label\n",
    "    }\n",
    "\n",
    "# Convert data to JSON string\n",
    "json_string = json.dumps(data, indent=4)\n",
    "with open(\"dataset_1_test.json\", \"w\") as json_file:\n",
    "    json_file.write(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "# Iterate over IDs, texts, and labels simultaneously\n",
    "for id_, text, label in zip(case_ids_train, text_train, BIO_labels_train):\n",
    "    data[id_] = {\n",
    "        \"text\": text,\n",
    "        \"labels\": label\n",
    "    }\n",
    "\n",
    "# Convert data to JSON string\n",
    "json_string = json.dumps(data, indent=4)\n",
    "with open(\"dataset_1_train.json\", \"w\") as json_file:\n",
    "    json_file.write(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "# Iterate over IDs, texts, and labels simultaneously\n",
    "for id_, text, label in zip(case_ids_val, text_val, BIO_labels_val):\n",
    "    data[id_] = {\n",
    "        \"text\": text,\n",
    "        \"labels\": label\n",
    "    }\n",
    "\n",
    "# Convert data to JSON string\n",
    "json_string = json.dumps(data, indent=4)\n",
    "with open(\"dataset_1_val.json\", \"w\") as json_file:\n",
    "    json_file.write(json_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
